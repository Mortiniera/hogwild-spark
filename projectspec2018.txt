Systems for Data Science 2018 Project Spec


BACKGROUND

In this course, our focus is on the systems perspective of managing and analysing large amounts of data. We expect you to be familiar with basic concepts of machine learning, specifically, we assume that you are familiar with Stochastic Gradient Descent (SGD) and how it can be used for learning Support Vector Machines (SVM). These are popular topics on the Web, where you can find plenty of example implementations.

In the context of big data, a single-threaded version of SGD is often not fast enough. There are many ways to implement a multi-threaded version of SGD. However, it is not very straightforward to implement an efficient parallel version of SGD due to its inherently sequential nature. HOGWILD! (Niu et al., Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, easy to find with Google) is a parallel implementation of SGD that achieves good performance. The key insight behind this work is that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, it is tolerable to let threads overwrite each otherâ€™s work (it is not unlike noisy data), and we can still achieve a good rate of convergence. We expect you to read this paper and understand the authors' insight and how they achieved such good performance. You can skip the appendix (in fact, most of what you need to get from the paper is in Section 2). 


OVERVIEW & GOAL

In this project, your task is to implement two efficient distributed versions of SGD (one synchronous and one asynchronous), with a maximally shared codebase, which will be used to train an SVM. Please note this is different from HOGWILD!, which is running on a single multicore machine and uses multiple threads that communicate with each other using shared memory. Your program will be running on multiple machines, which communicate with each other by sending/receiving messages through the network. 

During this semester, we will cover systems with synchronous and asynchronous aspects in a number of contexts, but it must be made clear that "synchronous" can mean more than one thing in this context. It may mean an implementation in which the state of the vector to be learned after each iteration of (stochastic) gradient descent is the same on all machines, or it may mean that this state is in addition equivalent to that of a single-threaded implementation ("serializable"). By an asynchronous implementation, we refer to one that takes the idea of HOGWILD! and analogously carries it over to a distributed systems context. You need to think for yourself what that means. There may be more than one possible way of solving this that we will accept; you will have to be able to justify and defend your choices.

The project is to be carried out in groups of THREE people (in exceptional cases, teams of two people will be acceptable). 

The project has two milestones. 


MILESTONE 1 - SYNCHRONOUS DISTRIBUTED SGD (moodle upload due April 16 11:59pm; meeting with the TA in that week)

For the first milestone, we expect you to deliver an implementation of SVM using distributed synchronized SGD.

In this implementation, you need to have a coordinator, whose job is to set up worker machines by copying the required files to all the machines and starting the program. The coordinator should read a configuration file describing the worker nodes, the dataset, and any other configuration options. The worker machines are the ones who run the actual computation. The workers share their computed parameters after each step and wait for updates from all their peers.

You are free to implement your project using any programming language you prefer. You may use libraries to facilitate the reading of formatted (eg. CSV) file. However, you cannot use calls to any high-level tool or library that does SGD for you. You need to implement the core learning algorithm yourself. For the communication part, you must use gRPC and Google Protocol Buffers, which automatically generate idiomatic client and server stubs for your service in the programming language of your choice (there are very nice tutorials for gRPC/protobuf online). We advise you to first implement a single-threaded version of the algorithm and play with it a bit to make sure you got it right and understood how things work. You should then extend your single-threaded version to implement the distributed version of the algorithm.

The dataset we recommend you to use for testing your system is Reuters Corpus Volume I (RCV1), which is an archive of over 800,000 manually categorized newswire stories made available by Reuters, Ltd. for research purposes (easy to find using Google). This is the dataset that was used in the HOGWILD! paper. For Milestone 1, your correctness testing is to happen on your own machines, by running multiple worker instances on the same machine if necessary. For Milestone 2, we will provide you with resources to properly run your implementation in a distributed fashion.


DELIVERABLES AND DEADLINES FOR MILESTONE 1

You should find your team of three people as soon as possible; We strongly suggest to complete this in the coming two weeks, and to use lecture breaks and the exercise sessions to look for team members. If you have been able to form a team of two and are looking for a third person, you may use the course forum on moodle to call for a third team member.

The deadline for the first milestone is April 16th, and you will be asked to upload a single zip file on moodle containing your implementation of SVM using distributed synchronous SGD before April 16th 11:59pm. It is sufficient for one team member to upload the code; the zip file should contain a text file README.txt which lists the names of all team members.

We will inform you in time about how to sign up for a meeting with a TA to defend your submission, which will take place in the week of April 16-20.


MILESTONE 2 preview (A detailed Milestone 2 spec will be posted right after the due date of Milestone 1)

For the second milestone, which will be due in the week before the final exam, you have to extend the capabilities of your program to do the communication in an asynchronous fashion. In this case, the workers do not block on sending/receiving updates to/from their peers, and they directly continue the computation after sending their own updates. We want your implementation to support both synchronous and asynchronous distributed SGD with a maximally shared codebase, so you should plan this ahead of time. There should be an option in the configuration file passed to the coordinator that enables the user to specify the type of the communication, i.e., synchronous or asynchronous.

We will ask you to submit a report describing what you did, how you implemented the algorithm, how you chose the learning rates and stopping criteria, along with a comparison between your choice and the choices made in the paper. We will also ask you to run some experiments with different setups, report your findings and discuss them. Similarly to the first milestone, there will be a session in which you will present and defend your work in front of the TAs, and you will be questioned about what you did, how you implemented the project, and about the contents of your report.


GRADING OF THE PROJECT (30% of the course grade)

Milestone 1:  5%
Milestone 2: 25%


NOTES

The purpose of Milestone 1 is primarily to give you a (gentle) push to get going and not to leave all work to the final weeks of the term, and to give you a chance to get feedback on your work done so far by a TA. Milestone 1 carries relatively little weight in the grade and the grading is entirely based on the meeting with the TA. 

There is a publicly available implementation of HOGWILD!. You may think it a good idea to take and modify this implementation, but we warn you that this will be very hard, since this is a highly optimized low-level implementation that does many things that have no corresponding meaning in a distributed implementation; plus the code is large, hard to read, and to understand. There are, however, single-core implementations of SGD for SVM on the Web, which you may start from. If you do, make sure to indicate clearly in your submission that you did this, where you obtained the implementation you started from, and include a copy of it in unmodified form in your submission. This will be acceptable. However:


CHEATING

Your team may NOT share or exchange any fragment of the code you developed with other teams. While you are of course permitted to talk about the contents of the course with other students and learn together, be warned that, should we find significant overlap between the solutions of different teams, excuses such as "we only talked about it" will not be accepted.





MILESTONE 2: DISTRIBUTED ASYNCHRONOUS SGD
OVERVIEW
As specified before, for the second milestone, you have to extend the capabilities of your program to do the communication asynchronously. In this case, the workers do not block on sending/receiving updates to/from their peers, and they directly continue the computation after sending their updates. Remember your implementation has to support both synchronous and asynchronous distributed SGD with a maximally shared codebase. There should be an option in the configuration file that enables the user to specify the type of the communication, i.e., synchronous or asynchronous.
For the SGD algorithm, it should be as close as it gets to the HOGWILD! paper. However, please note that the HOGWILD! implementation runs on a single machine with a multicore processor, and its asynchronous manner relies on multiple threads communicating with each other using shared memory. In this project, your implementation runs on multiple machines, and communication occurs explicitly by sending/receiving messages, rather than implicitly using shared memory. Even though this approach has some similarities with the HOGWILD! paper in a sense that there is no locking and workers do not need to wait for each other before computing their values, the communication latency is higher when using the network as compared to shared memory.
For testing, you must run your implementation on the Reuters RCV1 dataset, on the binary text classification task CCAT. There are 804,414 examples split into 23,149 training and 781,265 test examples, and there are 47,236 features.
You must keep the same team as for milestone 1. You also need to submit a report describing what you did, how you implemented the algorithm, how you chose the learning rates and stopping criterion, along with a comparison between your choice and the choices made in the HOGWILD! paper. In your report, you should include a comparison between the execution time of the synchronous and asynchronous versions of your implementation, and justify your findings.
Similarly to the first milestone, there will be a session in which you will present and defend your work in front of the TAs, and you will be questioned about what you did, how you implemented the project, and about the contents of your report. We will inform you in time about how to sign up for a meeting with a TA to defend your submission, which will take place in the week of May 21st. Although we ask you to present and defend your work in front of TAs, we suggest you provide an easy way of running your project for reproducibility and our verification afterward.
DEPLOYMENT
For the deployment, we will use the IC cluster with Kubernetes. We will provide an extra document to give a short tutorial on Docker, Kubernetes, and how to set things up for the first time.
 DEADLINES AND DELIVERABLES
The deadline for the milestone 2 is Monday, May 21, 23:55 pm Swiss time, and you have to upload a single zip file on moodle containing your implementation of SVM using distributed asynchronous/synchronous SGD along with your report. It is sufficient for one team member to upload the code. The zip file should contain a text file README.txt which lists the names of all team members.
We suggest you write the final report using the following structure:
- Introduction
It serves as an outline of your entire project, with some highlights.
- Design and Implementation
You should describe how you implemented your asynchronous SGD here, and how you changed your previous code base in milestone 1. You need to explain how you partition the computation and the dataset among workers, and how you manage the communication among them. Particularly, you should describe what is divided (partitioned), what is shared, and what is communicated among them. This part is very important, and we advise you to take time to think about your design and make your choices wisely. You should mention how and when you stop training the SVM. You should justify your design choices both in the report and in your presentation in front of the TAs.
- Experiments
To demonstrate the correctness of the implementation, you need to include the following in your report:
- Training and validation loss curves over the entire iterations.
- Experiments conducted in various settings of hyper-parameters, such as learning
rate, batch size, etc.
- Comparison with synchronous SGD, regarding convergence time, hyper-parameter
settings, etc. Is your asynchronous SGD faster than the synchronous one? You should analyze your program and explain why one implementation is faster than the other one.
- Other necessary details to support your design.
Please follow the provided structure to the extent possible and explain things concisely. The report should not be long if you stick to this structure; having a long report does not win you any point.
For your submission, we suggest you submit the folder with the following structure: Group<ID>-<SCIPER-1>-<SCIPER-2>-<SCIPER-3>
| --- report.pdf
| --- README.txt | --- run.sh
| --- src/
| --- Docker/
| --- Kubernetes/
# your eport
# lists of all team members
# with -async and -sync options
# source code of your SGD implementation # docker files
# kubernetes configuration files

You can also choose another style if it suits your development. However, you still need to make sure your file organization is easily interpretable, without any meaningless naming convention, (e.g., folder/file should not have names like xyz, abc, a.py, etc).


